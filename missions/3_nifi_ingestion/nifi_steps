You'll need to stand up a data hub cluster with Nifi using Flow Management Light Duty.
You'll also "need" another data hub to use hive or impala (I chose hive)

The instructions here are terrible, here are the corrections:

Sources:

* S3
You won't have access to this bucket, so you'll have to wget all the individual files:

wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000000_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000001_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000002_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000003_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000004_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000005_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000006_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000007_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000008_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000009_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000010_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000011_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000012_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000013_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000014_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000015_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000016_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000017_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000018_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000019_0
wget https://gravity-data.s3.ca-central-1.amazonaws.com/measurements/000020_0


* HDFS
You won't be able to HDFS to these files, you'll have to use InvokeHTTP in nifi with OPEN operation & psuedo authentication, it should look like this:

http://ec2-3-239-11-101.compute-1.amazonaws.com:14000/webhdfs/v1/user/gravity/galaxies.csv?op=open&user.name=cnelson2

* RDBMS
You'll have to download the mysql jdbc jar onto each of the nifi nodes.   There may be a way to do this from the UI, I did it from the command line.

I unzipped the zip & copied the jar into /tmp and (I think) gave it 777 permissions.  The DBMS connection pool serivce in nifi won't enable until it is on all 3(?) nodes.
In the controller service use this:
connection URL:  jdbc:mysql://ec2-3-239-11-101.compute-1.amazonaws.com:3306/gravity
driver location:  file:///tmp/mysql-connector-java-8.0.24.jar
driver class name:  com.mysql.jdbc.Driver

Make sure you only execute on the primary node or else the SQL will run 3 times and you have duped up data.

* Text File on FTP
you actually need to use SFTP, and the files are located in the /ftp folder
